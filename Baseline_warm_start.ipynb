{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aafd7ffc",
   "metadata": {},
   "source": [
    "# ML BLOCKS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3cad5be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d5abf75f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "from torch_geometric.data import Data, DataLoader, Batch\n",
    "\n",
    "from catboost import CatBoostClassifier, Pool\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "import gzip\n",
    "import json\n",
    "\n",
    "from warm_start import prepare_statistics, prepare_xy, prepare_gru_dataset, GruEDModel, make_gru_loader\n",
    "from redundant_constraints import EdgeModel, GraphTrainer, build_grid_graph, plot_grid_graph, compute_dc_power_flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "02c9b388",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "if torch.backends.mps.is_available():\n",
    "    DEVICE = \"mps\"\n",
    "elif torch.cuda.is_available():\n",
    "    DEVICE = \"cuda\"\n",
    "else:\n",
    "    DEVICE = \"cpu\"\n",
    "\n",
    "print(f\"Using device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05e7051b",
   "metadata": {},
   "source": [
    "# CHOOSE CASE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aa27a73a",
   "metadata": {},
   "outputs": [],
   "source": [
    "CASE = \"case300\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2ddf7a41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of files: 364\n",
      "Initial data keys: ['SOURCE', 'Parameters', 'Generators', 'Transmission lines', 'Contingencies', 'Buses', 'Reserves']\n",
      "Historical data keys: ['Thermal production (MW)/g1', 'Thermal production (MW)/g2', 'Thermal production (MW)/g3', 'Thermal production (MW)/g4', 'Thermal production (MW)/g5', 'Thermal production (MW)/g6', 'Thermal production (MW)/g7', 'Thermal production (MW)/g8', 'Thermal production (MW)/g9', 'Thermal production (MW)/g10', 'Thermal production (MW)/g11', 'Thermal production (MW)/g12', 'Thermal production (MW)/g13', 'Thermal production (MW)/g14', 'Thermal production (MW)/g15', 'Thermal production (MW)/g16', 'Thermal production (MW)/g17', 'Thermal production (MW)/g18', 'Thermal production (MW)/g19', 'Thermal production (MW)/g20', 'Thermal production (MW)/g21', 'Thermal production (MW)/g22', 'Thermal production (MW)/g23', 'Thermal production (MW)/g24', 'Thermal production (MW)/g25', 'Thermal production (MW)/g26', 'Thermal production (MW)/g27', 'Thermal production (MW)/g28', 'Thermal production (MW)/g29', 'Thermal production (MW)/g30', 'Thermal production (MW)/g31', 'Thermal production (MW)/g32', 'Thermal production (MW)/g33', 'Thermal production (MW)/g34', 'Thermal production (MW)/g35', 'Thermal production (MW)/g36', 'Thermal production (MW)/g37', 'Thermal production (MW)/g38', 'Thermal production (MW)/g39', 'Thermal production (MW)/g40', 'Thermal production (MW)/g41', 'Thermal production (MW)/g42', 'Thermal production (MW)/g43', 'Thermal production (MW)/g44', 'Thermal production (MW)/g45', 'Thermal production (MW)/g46', 'Thermal production (MW)/g47', 'Thermal production (MW)/g48', 'Thermal production (MW)/g49', 'Thermal production (MW)/g50', 'Thermal production (MW)/g51', 'Thermal production (MW)/g52', 'Thermal production (MW)/g53', 'Thermal production (MW)/g54', 'Thermal production (MW)/g55', 'Thermal production (MW)/g56', 'Thermal production (MW)/g57', 'Thermal production (MW)/g58', 'Thermal production (MW)/g59', 'Thermal production (MW)/g60', 'Thermal production (MW)/g61', 'Thermal production (MW)/g62', 'Thermal production (MW)/g63', 'Thermal production (MW)/g64', 'Thermal production (MW)/g65', 'Thermal production (MW)/g66', 'Thermal production (MW)/g67', 'Thermal production (MW)/g68', 'Thermal production (MW)/g69', 'Thermal production cost ($)/g1', 'Thermal production cost ($)/g2', 'Thermal production cost ($)/g3', 'Thermal production cost ($)/g4', 'Thermal production cost ($)/g5', 'Thermal production cost ($)/g6', 'Thermal production cost ($)/g7', 'Thermal production cost ($)/g8', 'Thermal production cost ($)/g9', 'Thermal production cost ($)/g10', 'Thermal production cost ($)/g11', 'Thermal production cost ($)/g12', 'Thermal production cost ($)/g13', 'Thermal production cost ($)/g14', 'Thermal production cost ($)/g15', 'Thermal production cost ($)/g16', 'Thermal production cost ($)/g17', 'Thermal production cost ($)/g18', 'Thermal production cost ($)/g19', 'Thermal production cost ($)/g20', 'Thermal production cost ($)/g21', 'Thermal production cost ($)/g22', 'Thermal production cost ($)/g23', 'Thermal production cost ($)/g24', 'Thermal production cost ($)/g25', 'Thermal production cost ($)/g26', 'Thermal production cost ($)/g27', 'Thermal production cost ($)/g28', 'Thermal production cost ($)/g29', 'Thermal production cost ($)/g30', 'Thermal production cost ($)/g31', 'Thermal production cost ($)/g32', 'Thermal production cost ($)/g33', 'Thermal production cost ($)/g34', 'Thermal production cost ($)/g35', 'Thermal production cost ($)/g36', 'Thermal production cost ($)/g37', 'Thermal production cost ($)/g38', 'Thermal production cost ($)/g39', 'Thermal production cost ($)/g40', 'Thermal production cost ($)/g41', 'Thermal production cost ($)/g42', 'Thermal production cost ($)/g43', 'Thermal production cost ($)/g44', 'Thermal production cost ($)/g45', 'Thermal production cost ($)/g46', 'Thermal production cost ($)/g47', 'Thermal production cost ($)/g48', 'Thermal production cost ($)/g49', 'Thermal production cost ($)/g50', 'Thermal production cost ($)/g51', 'Thermal production cost ($)/g52', 'Thermal production cost ($)/g53', 'Thermal production cost ($)/g54', 'Thermal production cost ($)/g55', 'Thermal production cost ($)/g56', 'Thermal production cost ($)/g57', 'Thermal production cost ($)/g58', 'Thermal production cost ($)/g59', 'Thermal production cost ($)/g60', 'Thermal production cost ($)/g61', 'Thermal production cost ($)/g62', 'Thermal production cost ($)/g63', 'Thermal production cost ($)/g64', 'Thermal production cost ($)/g65', 'Thermal production cost ($)/g66', 'Thermal production cost ($)/g67', 'Thermal production cost ($)/g68', 'Thermal production cost ($)/g69', 'Is_on/g1', 'Is_on/g2', 'Is_on/g3', 'Is_on/g4', 'Is_on/g5', 'Is_on/g6', 'Is_on/g7', 'Is_on/g8', 'Is_on/g9', 'Is_on/g10', 'Is_on/g11', 'Is_on/g12', 'Is_on/g13', 'Is_on/g14', 'Is_on/g15', 'Is_on/g16', 'Is_on/g17', 'Is_on/g18', 'Is_on/g19', 'Is_on/g20', 'Is_on/g21', 'Is_on/g22', 'Is_on/g23', 'Is_on/g24', 'Is_on/g25', 'Is_on/g26', 'Is_on/g27', 'Is_on/g28', 'Is_on/g29', 'Is_on/g30', 'Is_on/g31', 'Is_on/g32', 'Is_on/g33', 'Is_on/g34', 'Is_on/g35', 'Is_on/g36', 'Is_on/g37', 'Is_on/g38', 'Is_on/g39', 'Is_on/g40', 'Is_on/g41', 'Is_on/g42', 'Is_on/g43', 'Is_on/g44', 'Is_on/g45', 'Is_on/g46', 'Is_on/g47', 'Is_on/g48', 'Is_on/g49', 'Is_on/g50', 'Is_on/g51', 'Is_on/g52', 'Is_on/g53', 'Is_on/g54', 'Is_on/g55', 'Is_on/g56', 'Is_on/g57', 'Is_on/g58', 'Is_on/g59', 'Is_on/g60', 'Is_on/g61', 'Is_on/g62', 'Is_on/g63', 'Is_on/g64', 'Is_on/g65', 'Is_on/g66', 'Is_on/g67', 'Is_on/g68', 'Is_on/g69', 'Switch_on/g1', 'Switch_on/g2', 'Switch_on/g3', 'Switch_on/g4', 'Switch_on/g5', 'Switch_on/g6', 'Switch_on/g7', 'Switch_on/g8', 'Switch_on/g9', 'Switch_on/g10', 'Switch_on/g11', 'Switch_on/g12', 'Switch_on/g13', 'Switch_on/g14', 'Switch_on/g15', 'Switch_on/g16', 'Switch_on/g17', 'Switch_on/g18', 'Switch_on/g19', 'Switch_on/g20', 'Switch_on/g21', 'Switch_on/g22', 'Switch_on/g23', 'Switch_on/g24', 'Switch_on/g25', 'Switch_on/g26', 'Switch_on/g27', 'Switch_on/g28', 'Switch_on/g29', 'Switch_on/g30', 'Switch_on/g31', 'Switch_on/g32', 'Switch_on/g33', 'Switch_on/g34', 'Switch_on/g35', 'Switch_on/g36', 'Switch_on/g37', 'Switch_on/g38', 'Switch_on/g39', 'Switch_on/g40', 'Switch_on/g41', 'Switch_on/g42', 'Switch_on/g43', 'Switch_on/g44', 'Switch_on/g45', 'Switch_on/g46', 'Switch_on/g47', 'Switch_on/g48', 'Switch_on/g49', 'Switch_on/g50', 'Switch_on/g51', 'Switch_on/g52', 'Switch_on/g53', 'Switch_on/g54', 'Switch_on/g55', 'Switch_on/g56', 'Switch_on/g57', 'Switch_on/g58', 'Switch_on/g59', 'Switch_on/g60', 'Switch_on/g61', 'Switch_on/g62', 'Switch_on/g63', 'Switch_on/g64', 'Switch_on/g65', 'Switch_on/g66', 'Switch_on/g67', 'Switch_on/g68', 'Switch_on/g69', 'Switch_off/g1', 'Switch_off/g2', 'Switch_off/g3', 'Switch_off/g4', 'Switch_off/g5', 'Switch_off/g6', 'Switch_off/g7', 'Switch_off/g8', 'Switch_off/g9', 'Switch_off/g10', 'Switch_off/g11', 'Switch_off/g12', 'Switch_off/g13', 'Switch_off/g14', 'Switch_off/g15', 'Switch_off/g16', 'Switch_off/g17', 'Switch_off/g18', 'Switch_off/g19', 'Switch_off/g20', 'Switch_off/g21', 'Switch_off/g22', 'Switch_off/g23', 'Switch_off/g24', 'Switch_off/g25', 'Switch_off/g26', 'Switch_off/g27', 'Switch_off/g28', 'Switch_off/g29', 'Switch_off/g30', 'Switch_off/g31', 'Switch_off/g32', 'Switch_off/g33', 'Switch_off/g34', 'Switch_off/g35', 'Switch_off/g36', 'Switch_off/g37', 'Switch_off/g38', 'Switch_off/g39', 'Switch_off/g40', 'Switch_off/g41', 'Switch_off/g42', 'Switch_off/g43', 'Switch_off/g44', 'Switch_off/g45', 'Switch_off/g46', 'Switch_off/g47', 'Switch_off/g48', 'Switch_off/g49', 'Switch_off/g50', 'Switch_off/g51', 'Switch_off/g52', 'Switch_off/g53', 'Switch_off/g54', 'Switch_off/g55', 'Switch_off/g56', 'Switch_off/g57', 'Switch_off/g58', 'Switch_off/g59', 'Switch_off/g60', 'Switch_off/g61', 'Switch_off/g62', 'Switch_off/g63', 'Switch_off/g64', 'Switch_off/g65', 'Switch_off/g66', 'Switch_off/g67', 'Switch_off/g68', 'Switch_off/g69', 'objective_value']\n"
     ]
    }
   ],
   "source": [
    "INITIAL_DATA_DIR = Path(\"dataset/initial_cases\") / CASE\n",
    "HIST_DATA_DIR = Path(\"dataset/solution_gurobi_cases\") / CASE\n",
    "\n",
    "print(\"Number of files:\", len(list(HIST_DATA_DIR.glob('*.npz'))))\n",
    "\n",
    "initial_files = sorted(INITIAL_DATA_DIR.glob(\"*.json.gz\"))\n",
    "hist_files = sorted(HIST_DATA_DIR.glob(\"*.npz\"))\n",
    "\n",
    "with gzip.open(initial_files[0], \"rt\", encoding=\"utf-8\") as gzfile:\n",
    "    first_initial_data = json.load(gzfile)\n",
    "print(\"Initial data keys:\", list(first_initial_data.keys()))\n",
    "    \n",
    "with np.load(hist_files[0], allow_pickle=True) as npzfile:\n",
    "    first_hist_data = npzfile\n",
    "print(\"Historical data keys:\", first_hist_data.files)\n",
    "\n",
    "# Create and plot a graph from the CASE data\n",
    "G, bus_idx = build_grid_graph(first_initial_data)\n",
    "\n",
    "# plot_grid_graph(G, bus_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b21f9fc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATASET\n",
    "N_total = len(list(HIST_DATA_DIR.glob('*.npz')))\n",
    "N_train = int(0.7 * N_total)\n",
    "N_val = int(0.15 * N_total)\n",
    "N_test = N_total - N_train - N_val\n",
    "\n",
    "# Split\n",
    "train_init = initial_files[:N_train]\n",
    "train_hist = hist_files[:N_train]\n",
    "\n",
    "val_init = initial_files[N_train:N_train + N_val]\n",
    "val_hist = hist_files[N_train:N_train + N_val]\n",
    "\n",
    "test_init = initial_files[N_train + N_val:]\n",
    "test_hist = hist_files[N_train + N_val:]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86e61655",
   "metadata": {},
   "source": [
    "# WARM START"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "29bb336b",
   "metadata": {},
   "outputs": [],
   "source": [
    "OUT_DIR = Path(\"warm_start_results\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c655c737",
   "metadata": {},
   "source": [
    "## Unit commitment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9e862ecd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 254/254 [00:11<00:00, 21.61it/s]\n",
      "100%|██████████| 54/54 [00:02<00:00, 21.32it/s]\n",
      "100%|██████████| 56/56 [00:02<00:00, 21.79it/s]\n"
     ]
    }
   ],
   "source": [
    "df_train = prepare_statistics(train_init, train_hist)\n",
    "df_val = prepare_statistics(val_init, val_hist)\n",
    "df_test = prepare_statistics(test_init, test_hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "62e67a9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = prepare_xy(df_train)\n",
    "X_val, y_val = prepare_xy(df_val)\n",
    "X_test, y_test = prepare_xy(df_test)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "24311702",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 0.6024407\ttest: 0.6097334\tbest: 0.6097334 (0)\ttotal: 82.1ms\tremaining: 8.13s\n",
      "20:\tlearn: 0.0860404\ttest: 0.1347546\tbest: 0.1347546 (20)\ttotal: 461ms\tremaining: 1.73s\n",
      "40:\tlearn: 0.0313871\ttest: 0.0934609\tbest: 0.0934609 (40)\ttotal: 905ms\tremaining: 1.3s\n",
      "60:\tlearn: 0.0216199\ttest: 0.0931540\tbest: 0.0919851 (46)\ttotal: 1.37s\tremaining: 874ms\n",
      "80:\tlearn: 0.0186832\ttest: 0.0973095\tbest: 0.0919851 (46)\ttotal: 1.72s\tremaining: 404ms\n",
      "99:\tlearn: 0.0172483\ttest: 0.0996003\tbest: 0.0919851 (46)\ttotal: 2.06s\tremaining: 0us\n",
      "\n",
      "bestTest = 0.09198510884\n",
      "bestIteration = 46\n",
      "\n",
      "Shrink model to first 47 iterations.\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.70      0.90      0.78     10809\n",
      "         1.0       0.99      0.97      0.98    128295\n",
      "\n",
      "    accuracy                           0.96    139104\n",
      "   macro avg       0.84      0.93      0.88    139104\n",
      "weighted avg       0.97      0.96      0.96    139104\n",
      "\n"
     ]
    }
   ],
   "source": [
    "uc_model = CatBoostClassifier(\n",
    "    iterations=100,\n",
    "    depth=6,\n",
    "    learning_rate=0.1,\n",
    "    loss_function='Logloss',\n",
    "    verbose=20)\n",
    "\n",
    "uc_model.fit(X_train_scaled, y_train, eval_set=(X_val_scaled, y_val))\n",
    "\n",
    "y_pred = uc_model.predict(X_test_scaled)\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd688e91",
   "metadata": {},
   "source": [
    "## Energy dispatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "481ae12e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 254/254 [00:05<00:00, 44.78it/s]\n",
      "100%|██████████| 54/54 [00:01<00:00, 44.81it/s]\n",
      "100%|██████████| 56/56 [00:01<00:00, 46.28it/s]\n"
     ]
    }
   ],
   "source": [
    "X_seq_train, Y_train_seq = prepare_gru_dataset(train_init, train_hist, y_train.values)\n",
    "X_seq_val, Y_val_seq = prepare_gru_dataset(val_init, val_hist, y_val.values)\n",
    "X_seq_test, Y_test_seq = prepare_gru_dataset(test_init, test_hist, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2b77ba3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_static_train = df_train[df_train[\"hour\"] == 0][[\"gen_id\", \"pmin\", \"pmax\", \"avg_cost_per_mw\",\n",
    "                                                  \"avg_startup_cost_per_hour\", \"startup_delay_min\", \"startup_delay_max\", \n",
    "                                                  \"ramp_up\", \"ramp_down\", \"startup_limit\", \"shutdown_limit\",\n",
    "                                                  \"min_uptime\", \"min_downtime\"\n",
    "                                                  ]].values\n",
    "\n",
    "X_static_val = df_val[df_val[\"hour\"] == 0][[\"gen_id\", \"pmin\", \"pmax\", \"avg_cost_per_mw\",\n",
    "                                            \"avg_startup_cost_per_hour\", \"startup_delay_min\", \"startup_delay_max\", \n",
    "                                            \"ramp_up\", \"ramp_down\", \"startup_limit\", \"shutdown_limit\",\n",
    "                                            \"min_uptime\", \"min_downtime\"\n",
    "                                            ]].values\n",
    "X_static_test = df_test[df_test[\"hour\"] == 0][[\"gen_id\", \"pmin\", \"pmax\", \"avg_cost_per_mw\",\n",
    "                                                \"avg_startup_cost_per_hour\", \"startup_delay_min\", \"startup_delay_max\", \n",
    "                                                \"ramp_up\", \"ramp_down\", \"startup_limit\", \"shutdown_limit\",\n",
    "                                                \"min_uptime\", \"min_downtime\"\n",
    "                                                ]].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d4aa0680",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler_static = StandardScaler()\n",
    "\n",
    "X_static_train_scaled = scaler_static.fit_transform(X_static_train)\n",
    "X_static_val_scaled = scaler_static.transform(X_static_val)\n",
    "X_static_test_scaled = scaler_static.transform(X_static_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "404bcce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = make_gru_loader(X_seq_train, X_static_train_scaled, Y_train_seq)\n",
    "val_loader = make_gru_loader(X_seq_val, X_static_val_scaled, Y_val_seq, shuffle=False)\n",
    "test_loader = make_gru_loader(X_seq_test, X_static_test_scaled, Y_test_seq, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d095d7f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GruEDModel(seq_input_dim=3, static_input_dim=X_static_train.shape[1]).to(DEVICE)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "aae9b705",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='mps:0',\n",
      "       grad_fn=<TransposeBackward1>)\n",
      "tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='mps:0',\n",
      "       grad_fn=<TransposeBackward1>)\n",
      "tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='mps:0',\n",
      "       grad_fn=<TransposeBackward1>)\n",
      "tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='mps:0',\n",
      "       grad_fn=<TransposeBackward1>)\n",
      "tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='mps:0',\n",
      "       grad_fn=<TransposeBackward1>)\n",
      "tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='mps:0',\n",
      "       grad_fn=<TransposeBackward1>)\n",
      "tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='mps:0',\n",
      "       grad_fn=<TransposeBackward1>)\n",
      "tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='mps:0',\n",
      "       grad_fn=<TransposeBackward1>)\n",
      "tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='mps:0',\n",
      "       grad_fn=<TransposeBackward1>)\n",
      "tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='mps:0',\n",
      "       grad_fn=<TransposeBackward1>)\n",
      "tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='mps:0',\n",
      "       grad_fn=<TransposeBackward1>)\n",
      "tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='mps:0',\n",
      "       grad_fn=<TransposeBackward1>)\n",
      "tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='mps:0',\n",
      "       grad_fn=<TransposeBackward1>)\n",
      "tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='mps:0',\n",
      "       grad_fn=<TransposeBackward1>)\n",
      "tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='mps:0',\n",
      "       grad_fn=<TransposeBackward1>)\n",
      "tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='mps:0',\n",
      "       grad_fn=<TransposeBackward1>)\n",
      "tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='mps:0',\n",
      "       grad_fn=<TransposeBackward1>)\n",
      "tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='mps:0',\n",
      "       grad_fn=<TransposeBackward1>)\n",
      "tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='mps:0',\n",
      "       grad_fn=<TransposeBackward1>)\n",
      "tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='mps:0',\n",
      "       grad_fn=<TransposeBackward1>)\n",
      "tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='mps:0',\n",
      "       grad_fn=<TransposeBackward1>)\n",
      "tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='mps:0',\n",
      "       grad_fn=<TransposeBackward1>)\n",
      "tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='mps:0',\n",
      "       grad_fn=<TransposeBackward1>)\n",
      "tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='mps:0',\n",
      "       grad_fn=<TransposeBackward1>)\n",
      "tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='mps:0',\n",
      "       grad_fn=<TransposeBackward1>)\n",
      "tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='mps:0',\n",
      "       grad_fn=<TransposeBackward1>)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m x_seq, x_static, y \u001b[38;5;241m=\u001b[39m x_seq\u001b[38;5;241m.\u001b[39mto(DEVICE), x_static\u001b[38;5;241m.\u001b[39mto(DEVICE), y\u001b[38;5;241m.\u001b[39mto(DEVICE)\n\u001b[1;32m      8\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m----> 9\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_seq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_static\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(output, y)\n\u001b[1;32m     11\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/SCUC/SCUCa/warm_start/model.py:25\u001b[0m, in \u001b[0;36mGruEDModel.forward\u001b[0;34m(self, x_seq, x_static)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x_seq, x_static):\n\u001b[1;32m     24\u001b[0m     seq_out, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgru(x_seq) \n\u001b[0;32m---> 25\u001b[0m     \u001b[38;5;28;43mprint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mseq_out\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m     static_proj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstatic_fc(x_static)\n\u001b[1;32m     29\u001b[0m     static_proj \u001b[38;5;241m=\u001b[39m static_proj\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mexpand(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, seq_out\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m1\u001b[39m), \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/_tensor.py:431\u001b[0m, in \u001b[0;36mTensor.__repr__\u001b[0;34m(self, tensor_contents)\u001b[0m\n\u001b[1;32m    427\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    428\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__repr__\u001b[39m, (\u001b[38;5;28mself\u001b[39m,), \u001b[38;5;28mself\u001b[39m, tensor_contents\u001b[38;5;241m=\u001b[39mtensor_contents\n\u001b[1;32m    429\u001b[0m     )\n\u001b[1;32m    430\u001b[0m \u001b[38;5;66;03m# All strings are unicode in Python 3.\u001b[39;00m\n\u001b[0;32m--> 431\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_tensor_str\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_str\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensor_contents\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_contents\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/_tensor_str.py:664\u001b[0m, in \u001b[0;36m_str\u001b[0;34m(self, tensor_contents)\u001b[0m\n\u001b[1;32m    662\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad(), torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39m_python_dispatch\u001b[38;5;241m.\u001b[39m_disable_current_modes():\n\u001b[1;32m    663\u001b[0m     guard \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_DisableFuncTorch()\n\u001b[0;32m--> 664\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_str_intern\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensor_contents\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_contents\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/_tensor_str.py:595\u001b[0m, in \u001b[0;36m_str_intern\u001b[0;34m(inp, tensor_contents)\u001b[0m\n\u001b[1;32m    593\u001b[0m                     tensor_str \u001b[38;5;241m=\u001b[39m _tensor_str(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mto_dense(), indent)\n\u001b[1;32m    594\u001b[0m                 \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 595\u001b[0m                     tensor_str \u001b[38;5;241m=\u001b[39m \u001b[43m_tensor_str\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindent\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    597\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayout \u001b[38;5;241m!=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstrided:\n\u001b[1;32m    598\u001b[0m     suffixes\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlayout=\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayout))\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/_tensor_str.py:348\u001b[0m, in \u001b[0;36m_tensor_str\u001b[0;34m(self, indent)\u001b[0m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    347\u001b[0m     formatter \u001b[38;5;241m=\u001b[39m _Formatter(get_summarized_data(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m summarize \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m--> 348\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_tensor_str_with_formatter\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msummarize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mformatter\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/_tensor_str.py:283\u001b[0m, in \u001b[0;36m_tensor_str_with_formatter\u001b[0;34m(self, indent, summarize, formatter1, formatter2)\u001b[0m\n\u001b[1;32m    272\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _vector_str(\u001b[38;5;28mself\u001b[39m, indent, summarize, formatter1, formatter2)\n\u001b[1;32m    274\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m summarize \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m PRINT_OPTS\u001b[38;5;241m.\u001b[39medgeitems:\n\u001b[1;32m    275\u001b[0m     slices \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    276\u001b[0m         [\n\u001b[1;32m    277\u001b[0m             _tensor_str_with_formatter(\n\u001b[1;32m    278\u001b[0m                 \u001b[38;5;28mself\u001b[39m[i], indent \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m, summarize, formatter1, formatter2\n\u001b[1;32m    279\u001b[0m             )\n\u001b[1;32m    280\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, PRINT_OPTS\u001b[38;5;241m.\u001b[39medgeitems)\n\u001b[1;32m    281\u001b[0m         ]\n\u001b[1;32m    282\u001b[0m         \u001b[38;5;241m+\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m...\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m--> 283\u001b[0m         \u001b[38;5;241m+\u001b[39m [\n\u001b[1;32m    284\u001b[0m             _tensor_str_with_formatter(\n\u001b[1;32m    285\u001b[0m                 \u001b[38;5;28mself\u001b[39m[i], indent \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m, summarize, formatter1, formatter2\n\u001b[1;32m    286\u001b[0m             )\n\u001b[1;32m    287\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m PRINT_OPTS\u001b[38;5;241m.\u001b[39medgeitems, \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m))\n\u001b[1;32m    288\u001b[0m         ]\n\u001b[1;32m    289\u001b[0m     )\n\u001b[1;32m    290\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    291\u001b[0m     slices \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    292\u001b[0m         _tensor_str_with_formatter(\n\u001b[1;32m    293\u001b[0m             \u001b[38;5;28mself\u001b[39m[i], indent \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m, summarize, formatter1, formatter2\n\u001b[1;32m    294\u001b[0m         )\n\u001b[1;32m    295\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m))\n\u001b[1;32m    296\u001b[0m     ]\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/_tensor_str.py:284\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    272\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _vector_str(\u001b[38;5;28mself\u001b[39m, indent, summarize, formatter1, formatter2)\n\u001b[1;32m    274\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m summarize \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m PRINT_OPTS\u001b[38;5;241m.\u001b[39medgeitems:\n\u001b[1;32m    275\u001b[0m     slices \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    276\u001b[0m         [\n\u001b[1;32m    277\u001b[0m             _tensor_str_with_formatter(\n\u001b[1;32m    278\u001b[0m                 \u001b[38;5;28mself\u001b[39m[i], indent \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m, summarize, formatter1, formatter2\n\u001b[1;32m    279\u001b[0m             )\n\u001b[1;32m    280\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, PRINT_OPTS\u001b[38;5;241m.\u001b[39medgeitems)\n\u001b[1;32m    281\u001b[0m         ]\n\u001b[1;32m    282\u001b[0m         \u001b[38;5;241m+\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m...\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    283\u001b[0m         \u001b[38;5;241m+\u001b[39m [\n\u001b[0;32m--> 284\u001b[0m             \u001b[43m_tensor_str_with_formatter\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    285\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindent\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msummarize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mformatter1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mformatter2\u001b[49m\n\u001b[1;32m    286\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    287\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m PRINT_OPTS\u001b[38;5;241m.\u001b[39medgeitems, \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m))\n\u001b[1;32m    288\u001b[0m         ]\n\u001b[1;32m    289\u001b[0m     )\n\u001b[1;32m    290\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    291\u001b[0m     slices \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    292\u001b[0m         _tensor_str_with_formatter(\n\u001b[1;32m    293\u001b[0m             \u001b[38;5;28mself\u001b[39m[i], indent \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m, summarize, formatter1, formatter2\n\u001b[1;32m    294\u001b[0m         )\n\u001b[1;32m    295\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m))\n\u001b[1;32m    296\u001b[0m     ]\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/_tensor_str.py:283\u001b[0m, in \u001b[0;36m_tensor_str_with_formatter\u001b[0;34m(self, indent, summarize, formatter1, formatter2)\u001b[0m\n\u001b[1;32m    272\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _vector_str(\u001b[38;5;28mself\u001b[39m, indent, summarize, formatter1, formatter2)\n\u001b[1;32m    274\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m summarize \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m PRINT_OPTS\u001b[38;5;241m.\u001b[39medgeitems:\n\u001b[1;32m    275\u001b[0m     slices \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    276\u001b[0m         [\n\u001b[1;32m    277\u001b[0m             _tensor_str_with_formatter(\n\u001b[1;32m    278\u001b[0m                 \u001b[38;5;28mself\u001b[39m[i], indent \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m, summarize, formatter1, formatter2\n\u001b[1;32m    279\u001b[0m             )\n\u001b[1;32m    280\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, PRINT_OPTS\u001b[38;5;241m.\u001b[39medgeitems)\n\u001b[1;32m    281\u001b[0m         ]\n\u001b[1;32m    282\u001b[0m         \u001b[38;5;241m+\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m...\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m--> 283\u001b[0m         \u001b[38;5;241m+\u001b[39m [\n\u001b[1;32m    284\u001b[0m             _tensor_str_with_formatter(\n\u001b[1;32m    285\u001b[0m                 \u001b[38;5;28mself\u001b[39m[i], indent \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m, summarize, formatter1, formatter2\n\u001b[1;32m    286\u001b[0m             )\n\u001b[1;32m    287\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m PRINT_OPTS\u001b[38;5;241m.\u001b[39medgeitems, \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m))\n\u001b[1;32m    288\u001b[0m         ]\n\u001b[1;32m    289\u001b[0m     )\n\u001b[1;32m    290\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    291\u001b[0m     slices \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    292\u001b[0m         _tensor_str_with_formatter(\n\u001b[1;32m    293\u001b[0m             \u001b[38;5;28mself\u001b[39m[i], indent \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m, summarize, formatter1, formatter2\n\u001b[1;32m    294\u001b[0m         )\n\u001b[1;32m    295\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m))\n\u001b[1;32m    296\u001b[0m     ]\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/_tensor_str.py:284\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    272\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _vector_str(\u001b[38;5;28mself\u001b[39m, indent, summarize, formatter1, formatter2)\n\u001b[1;32m    274\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m summarize \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m PRINT_OPTS\u001b[38;5;241m.\u001b[39medgeitems:\n\u001b[1;32m    275\u001b[0m     slices \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    276\u001b[0m         [\n\u001b[1;32m    277\u001b[0m             _tensor_str_with_formatter(\n\u001b[1;32m    278\u001b[0m                 \u001b[38;5;28mself\u001b[39m[i], indent \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m, summarize, formatter1, formatter2\n\u001b[1;32m    279\u001b[0m             )\n\u001b[1;32m    280\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, PRINT_OPTS\u001b[38;5;241m.\u001b[39medgeitems)\n\u001b[1;32m    281\u001b[0m         ]\n\u001b[1;32m    282\u001b[0m         \u001b[38;5;241m+\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m...\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    283\u001b[0m         \u001b[38;5;241m+\u001b[39m [\n\u001b[0;32m--> 284\u001b[0m             \u001b[43m_tensor_str_with_formatter\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    285\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindent\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msummarize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mformatter1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mformatter2\u001b[49m\n\u001b[1;32m    286\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    287\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m PRINT_OPTS\u001b[38;5;241m.\u001b[39medgeitems, \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m))\n\u001b[1;32m    288\u001b[0m         ]\n\u001b[1;32m    289\u001b[0m     )\n\u001b[1;32m    290\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    291\u001b[0m     slices \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    292\u001b[0m         _tensor_str_with_formatter(\n\u001b[1;32m    293\u001b[0m             \u001b[38;5;28mself\u001b[39m[i], indent \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m, summarize, formatter1, formatter2\n\u001b[1;32m    294\u001b[0m         )\n\u001b[1;32m    295\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m))\n\u001b[1;32m    296\u001b[0m     ]\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/_tensor_str.py:272\u001b[0m, in \u001b[0;36m_tensor_str_with_formatter\u001b[0;34m(self, indent, summarize, formatter1, formatter2)\u001b[0m\n\u001b[1;32m    269\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _scalar_str(\u001b[38;5;28mself\u001b[39m, formatter1, formatter2)\n\u001b[1;32m    271\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m--> 272\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_vector_str\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msummarize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mformatter1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mformatter2\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m summarize \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m PRINT_OPTS\u001b[38;5;241m.\u001b[39medgeitems:\n\u001b[1;32m    275\u001b[0m     slices \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    276\u001b[0m         [\n\u001b[1;32m    277\u001b[0m             _tensor_str_with_formatter(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    288\u001b[0m         ]\n\u001b[1;32m    289\u001b[0m     )\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/_tensor_str.py:250\u001b[0m, in \u001b[0;36m_vector_str\u001b[0;34m(self, indent, summarize, formatter1, formatter2)\u001b[0m\n\u001b[1;32m    245\u001b[0m     data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m...\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    246\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m summarize \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m PRINT_OPTS\u001b[38;5;241m.\u001b[39medgeitems:\n\u001b[1;32m    247\u001b[0m     data \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    248\u001b[0m         [_val_formatter(val) \u001b[38;5;28;01mfor\u001b[39;00m val \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m[: PRINT_OPTS\u001b[38;5;241m.\u001b[39medgeitems]\u001b[38;5;241m.\u001b[39mtolist()]\n\u001b[1;32m    249\u001b[0m         \u001b[38;5;241m+\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m ...\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m--> 250\u001b[0m         \u001b[38;5;241m+\u001b[39m [_val_formatter(val) \u001b[38;5;28;01mfor\u001b[39;00m val \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43mPRINT_OPTS\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43medgeitems\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtolist\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m]\n\u001b[1;32m    251\u001b[0m     )\n\u001b[1;32m    252\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    253\u001b[0m     data \u001b[38;5;241m=\u001b[39m [_val_formatter(val) \u001b[38;5;28;01mfor\u001b[39;00m val \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtolist()]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "EPOCHS = 20\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    for x_seq, x_static, y in train_loader:\n",
    "        x_seq, x_static, y = x_seq.to(DEVICE), x_static.to(DEVICE), y.to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(x_seq, x_static)\n",
    "        loss = criterion(output, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item() * x_seq.size(0)\n",
    "\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for x_seq, x_static, y in val_loader:\n",
    "            x_seq, x_static, y = x_seq.to(DEVICE), x_static.to(DEVICE), y.to(DEVICE)\n",
    "            output = model(x_seq, x_static)\n",
    "            loss = criterion(output, y)\n",
    "            val_loss += loss.item() * x_seq.size(0)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{EPOCHS} | Train Loss: {train_loss/len(train_loader.dataset):.4f} | Val Loss: {val_loss/len(val_loader.dataset):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea6349e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "test_preds = []\n",
    "test_targets = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for x_seq, x_static, y in test_loader:\n",
    "        x_seq, x_static = x_seq.to(device), x_static.to(device)\n",
    "        output = model(x_seq, x_static)\n",
    "        test_preds.append(output.cpu())\n",
    "        test_targets.append(y)\n",
    "\n",
    "preds = torch.cat(test_preds).numpy()\n",
    "targets = torch.cat(test_targets).numpy()\n",
    "\n",
    "mse = np.mean((preds - targets) ** 2)\n",
    "print(\"Test MSE:\", mse)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77b8b448",
   "metadata": {},
   "source": [
    "# IDENTIFY REDUNDANT CONSTRAINTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5272a3b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d1ae638",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 364/364 [00:03<00:00, 105.20it/s]\n"
     ]
    }
   ],
   "source": [
    "# Power flow on each transmission line does not exceed its thermal limits\n",
    "LIMIT = 5.0\n",
    "\n",
    "# Statistic data for topology\n",
    "records = []\n",
    "\n",
    "for init_f, hist_f in tqdm(zip(initial_files, hist_files), total=len(hist_files)):\n",
    "    with gzip.open(init_f, \"rt\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "        \n",
    "    buses = data[\"Buses\"]\n",
    "    lines = data[\"Transmission lines\"]\n",
    "    gens  = data[\"Generators\"]\n",
    "    \n",
    "    n_bus = len(buses)\n",
    "    n_line = len(lines)\n",
    "    \n",
    "    bus_ids = list(buses.keys())\n",
    "    line_ids = list(lines.keys())\n",
    "    bus_idx = {bus_id: i for i, bus_id in enumerate(bus_ids)}\n",
    "    \n",
    "    # Transmission lines FEATURES (topology) - edge\n",
    "    edge_index_list = []\n",
    "    edge_attr_list = []\n",
    "\n",
    "    for line_id in line_ids:\n",
    "        line = lines[line_id]\n",
    "        i = bus_idx[line[\"Source bus\"]]\n",
    "        j = bus_idx[line[\"Target bus\"]]\n",
    "        reactance = line[\"Reactance (ohms)\"]\n",
    "        susceptance = line[\"Susceptance (S)\"]\n",
    "\n",
    "        edge_index_list.append([i, j])\n",
    "        edge_attr_list.append([reactance, susceptance])\n",
    "\n",
    "    edge_index = torch.tensor(edge_index_list).T\n",
    "    edge_attr = torch.tensor(edge_attr_list, dtype=torch.float32)\n",
    "    \n",
    "    # Buses features\n",
    "    x = []\n",
    "\n",
    "    for bus_id in bus_ids:\n",
    "        bus = buses[bus_id]\n",
    "        load = bus.get(\"Load (MW)\")\n",
    "\n",
    "        mean_load = np.mean(load)\n",
    "        max_load = np.max(load)\n",
    "        min_load = np.min(load)\n",
    "        std_load = np.std(load)\n",
    "\n",
    "        x.append([mean_load, max_load, min_load, std_load])\n",
    "\n",
    "    x = torch.tensor(x, dtype=torch.float32)\n",
    "\n",
    "    # TODO: Calculate power flow, take from historical data\n",
    "    # with np.load(hist_f, allow_pickle=True) as npzfile:\n",
    "        # hist_data = npzfile\n",
    "    \n",
    "    flows = compute_dc_power_flow(data)\n",
    "    \n",
    "    y = (flows >= LIMIT).astype(np.int64)\n",
    "    y = torch.tensor(y, dtype=torch.float32)\n",
    "    day_id = init_f.stem \n",
    "    \n",
    "    data = Data(x=x, edge_index=edge_index, edge_attr=edge_attr, y=y)\n",
    "    data.case_id = CASE\n",
    "    data.day_id  = day_id\n",
    "    data.edge_id = line_ids \n",
    "    records.append(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc8ede4b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'n_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m train_set, val_set, test_set \u001b[38;5;241m=\u001b[39m random_split(records, [\u001b[43mn_train\u001b[49m, n_val, n_test])\n\u001b[1;32m      3\u001b[0m loader_train \u001b[38;5;241m=\u001b[39m DataLoader(train_set, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m16\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      4\u001b[0m loader_val \u001b[38;5;241m=\u001b[39m DataLoader(val_set, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m16\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'n_train' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "train_set, val_set, test_set = random_split(records, [n_train, n_val, n_test])\n",
    "\n",
    "loader_train = DataLoader(train_set, batch_size=16, shuffle=True)\n",
    "loader_val = DataLoader(val_set, batch_size=16)\n",
    "loader_test = DataLoader(test_set, batch_size=n_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5f9f188",
   "metadata": {},
   "source": [
    "# ML model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8101a9d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n",
      "Epoch 000 | loss=0.7328 | acc=0.676 | prec=0.880 | rec=0.686 | f1=0.771\n",
      "Epoch 001 | loss=0.4709 | acc=0.819 | prec=0.931 | rec=0.834 | f1=0.880\n",
      "Epoch 002 | loss=0.4051 | acc=0.798 | prec=0.929 | rec=0.808 | f1=0.864\n",
      "Epoch 003 | loss=0.3799 | acc=0.836 | prec=0.931 | rec=0.858 | f1=0.893\n",
      "Epoch 004 | loss=0.3604 | acc=0.839 | prec=0.933 | rec=0.859 | f1=0.895\n",
      "Epoch 005 | loss=0.3444 | acc=0.880 | prec=0.941 | rec=0.906 | f1=0.923\n",
      "Epoch 006 | loss=0.3294 | acc=0.902 | prec=0.952 | rec=0.923 | f1=0.937\n",
      "Epoch 007 | loss=0.3159 | acc=0.909 | prec=0.955 | rec=0.930 | f1=0.942\n",
      "Epoch 008 | loss=0.3037 | acc=0.920 | prec=0.970 | rec=0.929 | f1=0.949\n",
      "Epoch 009 | loss=0.2924 | acc=0.917 | prec=0.973 | rec=0.921 | f1=0.946\n",
      "Epoch 010 | loss=0.2815 | acc=0.914 | prec=0.973 | rec=0.917 | f1=0.944\n",
      "Epoch 011 | loss=0.2705 | acc=0.915 | prec=0.973 | rec=0.919 | f1=0.945\n",
      "Epoch 012 | loss=0.2593 | acc=0.916 | prec=0.972 | rec=0.921 | f1=0.946\n",
      "Epoch 013 | loss=0.2493 | acc=0.929 | prec=0.973 | rec=0.936 | f1=0.954\n",
      "Epoch 014 | loss=0.2399 | acc=0.934 | prec=0.972 | rec=0.944 | f1=0.958\n",
      "Epoch 015 | loss=0.2308 | acc=0.942 | prec=0.981 | rec=0.945 | f1=0.963\n",
      "Epoch 016 | loss=0.2222 | acc=0.956 | prec=0.980 | rec=0.964 | f1=0.972\n",
      "Epoch 017 | loss=0.2138 | acc=0.963 | prec=0.981 | rec=0.972 | f1=0.977\n",
      "Epoch 018 | loss=0.2059 | acc=0.963 | prec=0.981 | rec=0.972 | f1=0.977\n",
      "Epoch 019 | loss=0.1987 | acc=0.965 | prec=0.985 | rec=0.971 | f1=0.978\n",
      "Epoch 020 | loss=0.1922 | acc=0.963 | prec=0.986 | rec=0.967 | f1=0.976\n",
      "Epoch 021 | loss=0.1863 | acc=0.959 | prec=0.988 | rec=0.960 | f1=0.974\n",
      "Epoch 022 | loss=0.1804 | acc=0.960 | prec=0.986 | rec=0.964 | f1=0.975\n",
      "Epoch 023 | loss=0.1749 | acc=0.962 | prec=0.986 | rec=0.966 | f1=0.976\n",
      "Epoch 024 | loss=0.1701 | acc=0.965 | prec=0.985 | rec=0.971 | f1=0.978\n",
      "Epoch 025 | loss=0.1655 | acc=0.962 | prec=0.986 | rec=0.966 | f1=0.976\n",
      "Epoch 026 | loss=0.1610 | acc=0.958 | prec=0.993 | rec=0.955 | f1=0.973\n",
      "Epoch 027 | loss=0.1583 | acc=0.946 | prec=0.995 | rec=0.937 | f1=0.965\n",
      "Epoch 028 | loss=0.1541 | acc=0.962 | prec=0.995 | rec=0.957 | f1=0.976\n",
      "Epoch 029 | loss=0.1508 | acc=0.962 | prec=0.995 | rec=0.957 | f1=0.976\n",
      "Epoch 030 | loss=0.1487 | acc=0.946 | prec=0.995 | rec=0.937 | f1=0.965\n",
      "Epoch 031 | loss=0.1454 | acc=0.959 | prec=0.995 | rec=0.953 | f1=0.974\n",
      "Epoch 032 | loss=0.1433 | acc=0.964 | prec=0.994 | rec=0.960 | f1=0.977\n",
      "Epoch 033 | loss=0.1416 | acc=0.938 | prec=0.995 | rec=0.927 | f1=0.960\n",
      "Epoch 034 | loss=0.1388 | acc=0.956 | prec=0.995 | rec=0.950 | f1=0.972\n",
      "Epoch 035 | loss=0.1375 | acc=0.960 | prec=0.994 | rec=0.956 | f1=0.974\n",
      "Epoch 036 | loss=0.1366 | acc=0.945 | prec=0.995 | rec=0.936 | f1=0.965\n",
      "Epoch 037 | loss=0.1361 | acc=0.947 | prec=0.995 | rec=0.938 | f1=0.966\n",
      "Epoch 038 | loss=0.1335 | acc=0.931 | prec=0.995 | rec=0.917 | f1=0.955\n",
      "Epoch 039 | loss=0.1321 | acc=0.944 | prec=0.995 | rec=0.934 | f1=0.963\n",
      "Epoch 040 | loss=0.1313 | acc=0.942 | prec=0.995 | rec=0.931 | f1=0.962\n",
      "Epoch 041 | loss=0.1304 | acc=0.964 | prec=0.994 | rec=0.960 | f1=0.977\n",
      "Epoch 042 | loss=0.1302 | acc=0.930 | prec=0.995 | rec=0.916 | f1=0.954\n",
      "Epoch 043 | loss=0.1292 | acc=0.911 | prec=0.995 | rec=0.893 | f1=0.941\n",
      "Epoch 044 | loss=0.1275 | acc=0.936 | prec=0.995 | rec=0.924 | f1=0.958\n",
      "Epoch 045 | loss=0.1263 | acc=0.935 | prec=0.995 | rec=0.923 | f1=0.958\n",
      "Epoch 046 | loss=0.1258 | acc=0.947 | prec=0.995 | rec=0.938 | f1=0.966\n",
      "Epoch 047 | loss=0.1243 | acc=0.938 | prec=0.995 | rec=0.927 | f1=0.960\n",
      "Epoch 048 | loss=0.1239 | acc=0.948 | prec=0.995 | rec=0.939 | f1=0.966\n",
      "Epoch 049 | loss=0.1234 | acc=0.908 | prec=0.995 | rec=0.889 | f1=0.939\n",
      "Epoch 050 | loss=0.1239 | acc=0.933 | prec=0.995 | rec=0.921 | f1=0.956\n",
      "Epoch 051 | loss=0.1226 | acc=0.944 | prec=0.995 | rec=0.935 | f1=0.964\n",
      "Epoch 052 | loss=0.1229 | acc=0.957 | prec=0.994 | rec=0.952 | f1=0.973\n",
      "Epoch 053 | loss=0.1204 | acc=0.948 | prec=0.995 | rec=0.939 | f1=0.966\n",
      "Epoch 054 | loss=0.1197 | acc=0.919 | prec=0.995 | rec=0.903 | f1=0.947\n",
      "Epoch 055 | loss=0.1199 | acc=0.950 | prec=0.995 | rec=0.942 | f1=0.968\n",
      "Epoch 056 | loss=0.1198 | acc=0.941 | prec=0.995 | rec=0.930 | f1=0.961\n",
      "Epoch 057 | loss=0.1195 | acc=0.964 | prec=0.994 | rec=0.960 | f1=0.977\n",
      "Epoch 058 | loss=0.1185 | acc=0.927 | prec=0.995 | rec=0.913 | f1=0.952\n",
      "Epoch 059 | loss=0.1178 | acc=0.914 | prec=0.995 | rec=0.896 | f1=0.943\n",
      "Epoch 060 | loss=0.1177 | acc=0.954 | prec=0.995 | rec=0.946 | f1=0.970\n",
      "Epoch 061 | loss=0.1169 | acc=0.937 | prec=0.995 | rec=0.925 | f1=0.959\n",
      "Epoch 062 | loss=0.1161 | acc=0.945 | prec=0.995 | rec=0.936 | f1=0.965\n",
      "Epoch 063 | loss=0.1156 | acc=0.910 | prec=0.995 | rec=0.892 | f1=0.940\n",
      "Epoch 064 | loss=0.1160 | acc=0.944 | prec=0.995 | rec=0.935 | f1=0.964\n",
      "Epoch 065 | loss=0.1150 | acc=0.945 | prec=0.995 | rec=0.936 | f1=0.965\n",
      "Epoch 066 | loss=0.1154 | acc=0.950 | prec=0.995 | rec=0.942 | f1=0.968\n",
      "Epoch 067 | loss=0.1157 | acc=0.921 | prec=0.995 | rec=0.906 | f1=0.948\n",
      "Epoch 068 | loss=0.1149 | acc=0.945 | prec=0.995 | rec=0.936 | f1=0.965\n",
      "Epoch 069 | loss=0.1147 | acc=0.936 | prec=0.995 | rec=0.924 | f1=0.958\n",
      "Epoch 070 | loss=0.1136 | acc=0.922 | prec=0.995 | rec=0.907 | f1=0.949\n",
      "Epoch 071 | loss=0.1151 | acc=0.920 | prec=0.995 | rec=0.905 | f1=0.948\n",
      "Epoch 072 | loss=0.1136 | acc=0.921 | prec=0.995 | rec=0.906 | f1=0.948\n",
      "Epoch 073 | loss=0.1124 | acc=0.944 | prec=0.995 | rec=0.935 | f1=0.964\n",
      "Epoch 074 | loss=0.1123 | acc=0.929 | prec=0.995 | rec=0.915 | f1=0.953\n",
      "Epoch 075 | loss=0.1116 | acc=0.942 | prec=0.995 | rec=0.931 | f1=0.962\n",
      "Epoch 076 | loss=0.1118 | acc=0.946 | prec=0.995 | rec=0.937 | f1=0.965\n",
      "Epoch 077 | loss=0.1118 | acc=0.953 | prec=0.995 | rec=0.945 | f1=0.970\n",
      "Epoch 078 | loss=0.1116 | acc=0.922 | prec=0.995 | rec=0.907 | f1=0.949\n",
      "Epoch 079 | loss=0.1106 | acc=0.953 | prec=0.995 | rec=0.945 | f1=0.970\n",
      "Epoch 080 | loss=0.1110 | acc=0.942 | prec=0.995 | rec=0.931 | f1=0.962\n",
      "Epoch 081 | loss=0.1104 | acc=0.950 | prec=0.995 | rec=0.942 | f1=0.968\n",
      "Epoch 082 | loss=0.1107 | acc=0.947 | prec=0.995 | rec=0.938 | f1=0.966\n",
      "Epoch 083 | loss=0.1096 | acc=0.944 | prec=0.995 | rec=0.934 | f1=0.963\n",
      "Epoch 084 | loss=0.1094 | acc=0.933 | prec=0.995 | rec=0.921 | f1=0.956\n",
      "Epoch 085 | loss=0.1095 | acc=0.947 | prec=0.995 | rec=0.938 | f1=0.966\n",
      "Epoch 086 | loss=0.1092 | acc=0.944 | prec=0.995 | rec=0.934 | f1=0.963\n",
      "Epoch 087 | loss=0.1089 | acc=0.929 | prec=0.995 | rec=0.915 | f1=0.953\n",
      "Epoch 088 | loss=0.1096 | acc=0.914 | prec=0.995 | rec=0.896 | f1=0.943\n",
      "Epoch 089 | loss=0.1093 | acc=0.945 | prec=0.995 | rec=0.936 | f1=0.965\n",
      "Epoch 090 | loss=0.1079 | acc=0.953 | prec=0.995 | rec=0.945 | f1=0.970\n",
      "Epoch 091 | loss=0.1080 | acc=0.944 | prec=0.995 | rec=0.935 | f1=0.964\n",
      "Epoch 092 | loss=0.1083 | acc=0.944 | prec=0.995 | rec=0.935 | f1=0.964\n",
      "Epoch 093 | loss=0.1077 | acc=0.929 | prec=0.995 | rec=0.915 | f1=0.953\n",
      "Epoch 094 | loss=0.1082 | acc=0.949 | prec=0.995 | rec=0.941 | f1=0.967\n",
      "Epoch 095 | loss=0.1086 | acc=0.926 | prec=0.995 | rec=0.912 | f1=0.951\n",
      "Epoch 096 | loss=0.1078 | acc=0.950 | prec=0.995 | rec=0.942 | f1=0.968\n",
      "Epoch 097 | loss=0.1068 | acc=0.944 | prec=0.995 | rec=0.934 | f1=0.963\n",
      "Epoch 098 | loss=0.1066 | acc=0.949 | prec=0.995 | rec=0.941 | f1=0.967\n",
      "Epoch 099 | loss=0.1071 | acc=0.944 | prec=0.995 | rec=0.935 | f1=0.964\n"
     ]
    }
   ],
   "source": [
    "model = EdgeModel(\n",
    "    node_in_dim=records[0].x.size(1),\n",
    "    edge_in_dim=records[0].edge_attr.size(1)\n",
    "    ).to(DEVICE)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-4)\n",
    "bce_loss = nn.BCELoss()\n",
    "EPOCHS = 100\n",
    "\n",
    "trainer = GraphTrainer(\n",
    "    model=model,\n",
    "    optimizer=optimizer,\n",
    "    criterion=bce_loss,\n",
    "    loader_train=loader_train,\n",
    "    loader_val=loader_val,\n",
    "    epochs = EPOCHS,\n",
    "    device = DEVICE\n",
    "    )\n",
    "\n",
    "trainer.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "145c6adb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved → /Users/grigorii/Desktop/SCUC/SCUC_problem/SCUCa/constraints_line_case14_2017-06-24.json\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from pathlib import Path, PurePath\n",
    "import json\n",
    "\n",
    "batch = next(iter(loader_val))\n",
    "graph = random.choice(batch.to_data_list()).to(DEVICE)\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    y_prob = model(graph.x, graph.edge_index, graph.edge_attr).cpu()\n",
    "\n",
    "th = 0.7\n",
    "y_pred = (y_prob > th).int()\n",
    "\n",
    "\n",
    "# line_dict = {lid: (bit) for lid, bit in zip(graph.edge_id, y_pred.tolist())}\n",
    "line_dict = {lid: int(1 - bit)            # меняем 0 ↔ 1\n",
    "             for lid, bit in zip(graph.edge_id, y_pred.tolist())}\n",
    "\n",
    "payload = {\n",
    "    \"case\": graph.case_id,\n",
    "    \"day\":  graph.day_id,\n",
    "    \"line_pruning\": line_dict\n",
    "}\n",
    "\n",
    "out = Path(f\"constraints_line_{graph.case_id}_{PurePath(graph.day_id).stem}.json\")\n",
    "out.write_text(json.dumps(payload, indent=2), encoding=\"utf-8\")\n",
    "print(\"Saved →\", out.resolve())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90726ec3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'l1': 0,\n",
       " 'l2': 0,\n",
       " 'l3': 0,\n",
       " 'l4': 0,\n",
       " 'l5': 0,\n",
       " 'l6': 0,\n",
       " 'l7': 0,\n",
       " 'l8': 0,\n",
       " 'l9': 0,\n",
       " 'l10': 0,\n",
       " 'l11': 0,\n",
       " 'l12': 0,\n",
       " 'l13': 0,\n",
       " 'l14': 1,\n",
       " 'l15': 0,\n",
       " 'l16': 1,\n",
       " 'l17': 0,\n",
       " 'l18': 1,\n",
       " 'l19': 1,\n",
       " 'l20': 1}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "line_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1af82d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "945c70c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'l1': 0,\n",
       " 'l2': 0,\n",
       " 'l3': 1,\n",
       " 'l4': 1,\n",
       " 'l5': 0,\n",
       " 'l6': 1,\n",
       " 'l7': 0,\n",
       " 'l8': 0,\n",
       " 'l9': 0,\n",
       " 'l10': 0,\n",
       " 'l11': 0,\n",
       " 'l12': 0,\n",
       " 'l13': 0,\n",
       " 'l14': 0,\n",
       " 'l15': 0,\n",
       " 'l16': 0,\n",
       " 'l17': 0,\n",
       " 'l18': 0,\n",
       " 'l19': 0,\n",
       " 'l20': 0}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "line_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84d0d9f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.9994, 0.9977, 1.0000, 1.0000, 1.0000, 1.0000, 0.9941, 0.9999, 0.9420,\n",
       "        0.9797, 0.5657, 0.9693, 0.9749, 0.0393, 0.9997, 0.4234, 0.9954, 0.0409,\n",
       "        0.0338, 0.1715])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01b866e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚡  Graph info →  CASE: ['case14'],   DAY: ['2017-04-13.json']\n",
      "Predicted: [0.999376118183136, 0.9977452158927917, 1.0, 1.0, 0.9999717473983765, 1.0, 0.9941180944442749, 0.9999126195907593, 0.9420437812805176, 0.9796971678733826, 0.5656982660293579, 0.9692999720573425, 0.9749167561531067, 0.039265818893909454, 0.9997473359107971, 0.4233649969100952, 0.9953832030296326, 0.0408671498298645, 0.03383460268378258, 0.17145630717277527]\n",
      "True     : [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0]\n",
      "tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "batch = next(iter(loader_val))\n",
    "i = np.random.randint(0, batch.num_graphs - 1)\n",
    "single_graph = Batch.from_data_list([batch.get_example(i)]).to(DEVICE)\n",
    "\n",
    "print(f\"⚡  Graph info →  CASE: {single_graph.case_id},   DAY: {single_graph.day_id}\")\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    y_pred = model(single_graph.x, single_graph.edge_index, single_graph.edge_attr).cpu()\n",
    "    y_true = single_graph.y.cpu()\n",
    "\n",
    "threshold = 0.7\n",
    "y_bin = (y_pred > threshold).int()\n",
    "y_bin, y_true\n",
    "\n",
    "print(\"Predicted:\", y_pred.tolist())\n",
    "print(\"True     :\", y_true.tolist())\n",
    "print(sum(y_bin)/sum(y_true))  # Accuracy metric\n",
    "\n",
    "    #   ( (y_pred & y_true).sum().item() / max(1, y_true.sum().item()) ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74163a90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved → /Users/grigorii/Desktop/SCUC/SCUC_problem/SCUCa/pruning_case14_2017-07-20.json\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path, PurePath\n",
    "import json\n",
    "\n",
    "line_dict = {lid: int(bit) for lid, bit in zip(graph.edge_id, y_pred.tolist())}\n",
    "\n",
    "payload = {\n",
    "    \"case\": graph.case_id,\n",
    "    \"day\":  graph.day_id,\n",
    "    \"line_pruning\": line_dict\n",
    "}\n",
    "\n",
    "out = Path(f\"pruning_{graph.case_id}_{PurePath(graph.day_id).stem}.json\")\n",
    "out.write_text(json.dumps(payload, indent=2), encoding=\"utf-8\")\n",
    "print(\"Saved →\", out.resolve())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96aa0bc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/6v/trq398m145gbsl8204wvnzvh0000gn/T/ipykernel_41640/3601773704.py:54: UserWarning: Tight layout not applied. The left and right margins cannot be made large enough to accommodate all Axes decorations.\n",
      "  plt.tight_layout()\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "# Create figure\n",
    "fig, ax = plt.subplots(figsize=(12, 5))\n",
    "ax.axis('off')\n",
    "\n",
    "# Box specs\n",
    "box_w = 2.6\n",
    "box_h = 1.0\n",
    "y_level = 0.6\n",
    "\n",
    "labels = [\n",
    "    \"Identify Redundant\\nConstraints\",\n",
    "    \"Warm‑Start\\nPartial Solution\",\n",
    "    \"Fix Partial\\nSolution (Subspace)\",\n",
    "    \"Predict Integer\\nDecisions\"\n",
    "]\n",
    "\n",
    "boxes = []\n",
    "for i, label in enumerate(labels):\n",
    "    x = 0.4 + i * (box_w + 0.8)\n",
    "    rect = patches.FancyBboxPatch(\n",
    "        (x, y_level), box_w, box_h,\n",
    "        boxstyle=\"round,pad=0.2\", linewidth=1.5, edgecolor=\"steelblue\", facecolor=\"#EFF6FF\"\n",
    "    )\n",
    "    ax.add_patch(rect)\n",
    "    ax.text(x + box_w / 2, y_level + box_h / 2, label, ha='center', va='center', fontsize=10)\n",
    "    boxes.append(rect)\n",
    "\n",
    "# Arrows\n",
    "for i in range(len(labels) - 1):\n",
    "    x_start = boxes[i].get_x() + box_w\n",
    "    x_end = boxes[i+1].get_x()\n",
    "    y = y_level + box_h / 2\n",
    "    ax.annotate(\n",
    "        \"\",\n",
    "        xy=(x_end - 0.05, y), xytext=(x_start + 0.05, y),\n",
    "        arrowprops=dict(arrowstyle=\"->\", linewidth=1.4)\n",
    "    )\n",
    "\n",
    "# Input and output boxes\n",
    "in_rect = patches.FancyBboxPatch((-0.1, y_level), 0.3, box_h,\n",
    "                                 boxstyle=\"round,pad=0.2\", facecolor=\"#D1FAE5\", edgecolor=\"seagreen\")\n",
    "ax.add_patch(in_rect)\n",
    "ax.text(0.05, y_level + box_h / 2, \"New\\nSCUC\\nInstance\", ha=\"left\", va=\"center\", fontsize=9)\n",
    "\n",
    "out_rect = patches.FancyBboxPatch((boxes[-1].get_x() + box_w + 0.5, y_level), 1.7, box_h,\n",
    "                                  boxstyle=\"round,pad=0.2\", facecolor=\"#FDE68A\", edgecolor=\"darkorange\")\n",
    "ax.add_patch(out_rect)\n",
    "ax.text(out_rect.get_x() + 0.85, y_level + box_h / 2,\n",
    "        \"Reduced MILP\\n+ Solver\\n(10× faster)\", ha=\"center\", va=\"center\", fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
